{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Intro to Probability\n",
    "Notes by Gloria Ha (2019),  Mary Richardson (2020), and Colin Hemez (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Now that we've gone over how to do this theoretically, let's try it on some data! You can download the data from the links at the bottom of the section notes page. Let's take a look at the *sleek* data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rrsrrrrsrsrrsrr\r\n",
      "srrrrsrsssrss\r\n",
      "rsrsssrssrssrrrrs\r\n",
      "srrrrrrrrsrssrs\r\n",
      "rrsssrsssrssrs\r\n",
      "srssrsssrsssr\r\n",
      "srrsrrrsrsssrss\r\n",
      "srrrrrsrssrssrss\r\n",
      "srrssrssrssrs\r\n",
      "ssrrrrsrrrsrrrssrssrsr\r\n"
     ]
    }
   ],
   "source": [
    "! head sleek_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some sequences of 'r' and 's' of varying length!  The fluffy dataset is similar.  Note that each file has 1000 sequences.  We can now load in the data into lists (so we'll have lists of strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sleek and fluffy data lists\n",
    "sleek_data = []\n",
    "fluffy_data = []\n",
    "\n",
    "# Load data from files\n",
    "with open('sleek_data.txt') as sleek_file:\n",
    "    for line in sleek_file:\n",
    "        sleek_data.append(line[:-1])\n",
    "        \n",
    "with open('fluffy_data.txt') as fluffy_file:\n",
    "    for line in fluffy_file:\n",
    "        fluffy_data.append(line[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: See if you can use a simple zero order method to differentiate the birdsongs\n",
    "\n",
    "See if you can use a simple scoring algorithm (+1 for 's' and -1 for 'r', for example) on the *sleek* and *fluffy* data to differentiate the sequences. \n",
    "\n",
    "I would recommend keeping two different arrays of length 1000: one for the scores you give to the sleek sequences, and one for the scores you gives to the fluffy sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store length of data\n",
    "num_seq = len(fluffy_data)\n",
    "\n",
    "# Initialize score arrays\n",
    "sleek_scores = np.zeros(num_seq)\n",
    "fluffy_scores = np.zeros(num_seq)\n",
    "\n",
    "# Write script to score sequences in both datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a score for every sequence in the dataset, try plotting a **histogram** of the distribution of scores for sleek and fluffy songs. Check out `plt.hist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try plotting a **ROC plot**. An easy way of going about this is to calculate the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for each threshold score that you're considering. \n",
    "\n",
    "I would recommend first defining a range of threshold score values (for example, the minimum and the maximum of all of your scores), defining a linear space between those values (check out `np.linspace`), and then calculating the TP, TN, FP, FN for each threshold value. In this case, let's say that \"positives\" are scores above the threshold, and \"negatives\" are scores below the threshold (this is the typical definition). \n",
    "\n",
    "Remember that the ROC plot is true positive rate $\\frac{TP}{TP+FN}$ vs. false positive rate $\\frac{FP}{TN+FP}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-bd94819724b5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-bd94819724b5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    min_score =\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Find minimum and maximum scores across both fluffy and sleek songs\n",
    "min_score = \n",
    "max_score = \n",
    "\n",
    "# Define number and range of threshold scores\n",
    "num_thresh = 1000\n",
    "threshold_scores = \n",
    "\n",
    "# Set up \n",
    "TP = np.zeros(num_thresh)\n",
    "FP = np.zeros(num_thresh)\n",
    "TN = np.zeros(num_thresh)\n",
    "FN = np.zeros(num_thresh)\n",
    "\n",
    "# Calculate TP,FP,TN,FN for each sequence in both datasets\n",
    "\n",
    "# Plot it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this method perform in distinguishing the songs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Train a third order Markov model\n",
    "\n",
    "#### Exercise 2.1: Split up your data into training and testing sets\n",
    "\n",
    "Now for the fun part!\n",
    "\n",
    "I would recommend first splitting up the data into **training and testing sets**. You can use half of the data for training and half for testing (like the homework), or choose some other fraction (as long as the training and testing sets don't overlap)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up training and testing sets\n",
    "sleek_test = \n",
    "sleek_train = \n",
    "fluffy_test = \n",
    "fluffy_train = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use our training dataset to calculate the parameters of our Markov model. There are many ways you could go about this, so don't feel like you have to follow these steps!\n",
    "\n",
    "As discussed in section, in order to calculate the **conditional probabilities** in the Markov model (ex: $P(s|srss)$), we can calculate the unconditional probabilities of the related 4-mers and 3-mers ($P(srss)$ and $P(srs)$ in this case).\n",
    "\n",
    "How would we go about calculating these probabilities?  We want to see how many times each 4-mer appears in the data, and how many times each 3-mer appears in the data, for both *sleek* and *fluffy* sequences.\n",
    "\n",
    "#### Exercise 2.2: How many 4-mers and 3-mers are possible in this dataset?\n",
    "\n",
    "One useful piece of information would be how many distinct 4-mers and 3-mers there are in the data.  We know that there are 2 states in this model: 's' and 'r'.  How many distinct 3-character and 4-character sequences are possible?  You could write them out by hand, or you could use probability (Hint: Does order matter? Are you sampling with replacement?)!\n",
    "\n",
    "#### Exercise 2.3: Store distinct 4-mers and 3-mers in a list\n",
    "\n",
    "Now we want to store the distinct 4-mers and 3-mers in a list. Something like `['ssss','sssr',...]` and `['sss','ssr',...]`.  Again, you could type out all possible combinations by hand, or you could harness the tools of Python to get all possible combinations (this will be even more useful with an alphabet of four letter like 'ACTG')! Check out `itertools`.  Again there are many ways to approach this, so feel free to diverge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all distinct 3-mer and 4-mer sequences and store them in lists\n",
    "threemers = \n",
    "fourmers = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to go through all of the training sequences and count how many times each 3-mer and 4-mer appears in the dataset.\n",
    "\n",
    "#### Exercise 2.4: Find frequencies of distinct 3/4-mers in training data\n",
    "\n",
    "Go through each sequence and identify each 3-mer and 4-mer, and update the counts of the corresponding 3-mers or 4-mers. Check out `list.index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counts array of 3-mers and 4-mers\n",
    "threemer_counts_sleek = \n",
    "threemer_counts_fluffy = \n",
    "fourmer_counts_sleek = \n",
    "fourmer_counts_fluffy = \n",
    "\n",
    "# Go through each training sequence and identify 3-mers and 4-mers and update counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can normalize the counts to get frequencies, which we'll use as probabilities in our Markov models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize counts to get frequencies/probabilities\n",
    "fourmer_prob_fluffy = \n",
    "fourmer_prob_sleek = \n",
    "threemer_prob_fluffy = \n",
    "threemer_prob_sleek = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained our models! We can simply divide the relevant 3-mer and 4-mer probabilities when calculating the conditional probabilities. All that's left to do is to test the model on distinguishing the testing set sequences as *sleek* or *fluffy*.\n",
    "\n",
    "### Exercise 3: Test the model\n",
    "\n",
    "Use the equation for a third order Markov chain probability to calculate the probability of each sequence given the two Markov models (*sleek* and *fluffy*). Use these probabilities to calculate the log odds score, where in this case we are dividing the probability of being *sleek* by the probability of being *fluffy*. Remember that we want to use **logs of probabilities** whenever we are multiplying/dividing them together to avoid **underflow** errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize score array\n",
    "fluffy_scores = np.zeros(len(fluffy_test))\n",
    "sleek_scores = np.zeros(len(sleek_test))\n",
    "\n",
    "# Iterate through each sequence\n",
    "\n",
    "    # For each sequence, calculate the initial probability \n",
    "    # of the first three notes under both the fluffy and sleek models\n",
    "\n",
    "    # For each sequence, calculate the probabilities of each note (r or s) given\n",
    "    # the previous three notes (starting with the fourth note) under both models\n",
    "\n",
    "    # For each sequence, calculate a log odds score and store it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have two arrays of scores, one for the *sleek* test dataset, and one for the *fluffy* test dataset. You can now repeat the histogram and ROC plot part of exercise 1 on these new scores. Did the model do any better? What's the **specificity** if you set a **sensitivity** threshold of 0.9 or higher? What if the *sleek* songs only made up 1% of the total songs? What would the **false discovery rate** be?\n",
    "\n",
    "### Notes\n",
    "\n",
    "For the purposes of this exercise, I split everything up and wrote a lot of repetitive code. In your actual homework, you can consider putting scripts into functions. If you're interested in doing more with this dataset, you could see if generating random note sequences would be distinguishable from *sleek* sequences under a zero order model. You could also see if these sequences could be better distinguished with any other order of Markov model (though, they were generated under a third order model...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
